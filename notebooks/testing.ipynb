{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:18: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[0].iloc[-h:] = nan\n",
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[0].iloc[-h:] = nan\n",
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:19: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[1].iloc[-h:] = nan\n",
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[1].iloc[-h:] = nan\n",
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:20: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['one'].iloc[-h:] = nan\n",
      "/var/folders/j5/td8zgnpj0xvc79yhsnbjr17m0000gn/T/ipykernel_61572/4140181271.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['one'].iloc[-h:] = nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>one</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.992353</td>\n",
       "      <td>0.323965</td>\n",
       "      <td>-0.302274</td>\n",
       "      <td>-1.585237</td>\n",
       "      <td>-0.544800</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>-0.021441</td>\n",
       "      <td>0.316169</td>\n",
       "      <td>-0.098997</td>\n",
       "      <td>1.085379</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.888431</td>\n",
       "      <td>-0.119908</td>\n",
       "      <td>0.580787</td>\n",
       "      <td>-0.399038</td>\n",
       "      <td>1.200697</td>\n",
       "      <td>2.305707</td>\n",
       "      <td>0.531889</td>\n",
       "      <td>1.483389</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.378333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.298005</td>\n",
       "      <td>1.169075</td>\n",
       "      <td>0.078595</td>\n",
       "      <td>0.186472</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>-0.077167</td>\n",
       "      <td>0.935385</td>\n",
       "      <td>-0.559335</td>\n",
       "      <td>0.341219</td>\n",
       "      <td>0.803685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.879894</td>\n",
       "      <td>1.283240</td>\n",
       "      <td>0.275753</td>\n",
       "      <td>0.412148</td>\n",
       "      <td>-0.494896</td>\n",
       "      <td>2.457624</td>\n",
       "      <td>0.238578</td>\n",
       "      <td>-1.975909</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.055729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.786382</td>\n",
       "      <td>-0.463325</td>\n",
       "      <td>0.721203</td>\n",
       "      <td>1.348425</td>\n",
       "      <td>-0.501516</td>\n",
       "      <td>0.488126</td>\n",
       "      <td>1.598850</td>\n",
       "      <td>0.223581</td>\n",
       "      <td>-0.727041</td>\n",
       "      <td>-0.928016</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.998879</td>\n",
       "      <td>0.021455</td>\n",
       "      <td>-0.213586</td>\n",
       "      <td>0.838839</td>\n",
       "      <td>0.124605</td>\n",
       "      <td>-0.246737</td>\n",
       "      <td>-1.024014</td>\n",
       "      <td>0.612363</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.889502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.276180</td>\n",
       "      <td>0.545390</td>\n",
       "      <td>1.035878</td>\n",
       "      <td>-0.755436</td>\n",
       "      <td>0.430923</td>\n",
       "      <td>0.669475</td>\n",
       "      <td>-0.537089</td>\n",
       "      <td>0.515443</td>\n",
       "      <td>0.047906</td>\n",
       "      <td>-1.174832</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.110335</td>\n",
       "      <td>-0.416424</td>\n",
       "      <td>-0.862757</td>\n",
       "      <td>1.326787</td>\n",
       "      <td>0.690854</td>\n",
       "      <td>-0.906616</td>\n",
       "      <td>0.476509</td>\n",
       "      <td>0.699091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.072863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.259395</td>\n",
       "      <td>-1.594934</td>\n",
       "      <td>1.809248</td>\n",
       "      <td>1.002226</td>\n",
       "      <td>-0.815308</td>\n",
       "      <td>1.749017</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>-0.762308</td>\n",
       "      <td>0.363637</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.790596</td>\n",
       "      <td>0.929426</td>\n",
       "      <td>0.428156</td>\n",
       "      <td>-0.882503</td>\n",
       "      <td>0.082432</td>\n",
       "      <td>-1.320863</td>\n",
       "      <td>0.693618</td>\n",
       "      <td>1.953759</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.294877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.415712</td>\n",
       "      <td>1.443924</td>\n",
       "      <td>-0.920818</td>\n",
       "      <td>2.263012</td>\n",
       "      <td>1.571203</td>\n",
       "      <td>0.744302</td>\n",
       "      <td>-1.402939</td>\n",
       "      <td>-1.623268</td>\n",
       "      <td>2.487828</td>\n",
       "      <td>2.005971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068997</td>\n",
       "      <td>-0.620042</td>\n",
       "      <td>2.154509</td>\n",
       "      <td>-1.618542</td>\n",
       "      <td>0.785786</td>\n",
       "      <td>-1.704877</td>\n",
       "      <td>0.118322</td>\n",
       "      <td>0.522214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.051083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.182928</td>\n",
       "      <td>0.111827</td>\n",
       "      <td>-1.209468</td>\n",
       "      <td>0.021120</td>\n",
       "      <td>-1.185045</td>\n",
       "      <td>0.461112</td>\n",
       "      <td>-1.344325</td>\n",
       "      <td>-2.157952</td>\n",
       "      <td>0.752266</td>\n",
       "      <td>-0.063163</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.309927</td>\n",
       "      <td>-0.206375</td>\n",
       "      <td>1.288428</td>\n",
       "      <td>-1.122680</td>\n",
       "      <td>-0.191745</td>\n",
       "      <td>-0.129728</td>\n",
       "      <td>-0.132934</td>\n",
       "      <td>-0.470554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.206534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.641366</td>\n",
       "      <td>-0.289803</td>\n",
       "      <td>0.694779</td>\n",
       "      <td>1.942122</td>\n",
       "      <td>1.372276</td>\n",
       "      <td>-1.522415</td>\n",
       "      <td>-0.673062</td>\n",
       "      <td>0.305787</td>\n",
       "      <td>-0.121505</td>\n",
       "      <td>0.085683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865407</td>\n",
       "      <td>-0.557632</td>\n",
       "      <td>0.189961</td>\n",
       "      <td>0.819096</td>\n",
       "      <td>0.095489</td>\n",
       "      <td>1.006587</td>\n",
       "      <td>0.793813</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.180246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.191516</td>\n",
       "      <td>0.576992</td>\n",
       "      <td>-1.643182</td>\n",
       "      <td>-0.131669</td>\n",
       "      <td>0.343353</td>\n",
       "      <td>-1.711821</td>\n",
       "      <td>-0.443054</td>\n",
       "      <td>-0.634584</td>\n",
       "      <td>1.708412</td>\n",
       "      <td>1.385935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.949156</td>\n",
       "      <td>1.201457</td>\n",
       "      <td>1.199512</td>\n",
       "      <td>-0.423426</td>\n",
       "      <td>-0.421269</td>\n",
       "      <td>0.505954</td>\n",
       "      <td>-1.413370</td>\n",
       "      <td>-0.839522</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.540943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-2.311907</td>\n",
       "      <td>1.210484</td>\n",
       "      <td>0.801909</td>\n",
       "      <td>0.518531</td>\n",
       "      <td>-0.531715</td>\n",
       "      <td>-0.010624</td>\n",
       "      <td>-0.408591</td>\n",
       "      <td>-1.236370</td>\n",
       "      <td>-0.386644</td>\n",
       "      <td>-0.745807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290238</td>\n",
       "      <td>-0.467597</td>\n",
       "      <td>0.198447</td>\n",
       "      <td>0.575039</td>\n",
       "      <td>-0.325595</td>\n",
       "      <td>-1.779368</td>\n",
       "      <td>-1.016658</td>\n",
       "      <td>0.899743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.831396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.784797</td>\n",
       "      <td>-1.849507</td>\n",
       "      <td>-0.581304</td>\n",
       "      <td>-1.219007</td>\n",
       "      <td>0.180560</td>\n",
       "      <td>-1.475992</td>\n",
       "      <td>-1.396014</td>\n",
       "      <td>0.797542</td>\n",
       "      <td>-1.867400</td>\n",
       "      <td>-0.916721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514531</td>\n",
       "      <td>-1.546215</td>\n",
       "      <td>-0.365697</td>\n",
       "      <td>-1.512766</td>\n",
       "      <td>-0.365535</td>\n",
       "      <td>-0.509073</td>\n",
       "      <td>-2.063466</td>\n",
       "      <td>0.436263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.330382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.819581</td>\n",
       "      <td>-0.007059</td>\n",
       "      <td>-0.097815</td>\n",
       "      <td>1.864404</td>\n",
       "      <td>1.052482</td>\n",
       "      <td>0.423650</td>\n",
       "      <td>1.379318</td>\n",
       "      <td>-0.669502</td>\n",
       "      <td>2.061232</td>\n",
       "      <td>-1.467686</td>\n",
       "      <td>...</td>\n",
       "      <td>1.214861</td>\n",
       "      <td>1.343246</td>\n",
       "      <td>-0.648931</td>\n",
       "      <td>0.973022</td>\n",
       "      <td>1.266891</td>\n",
       "      <td>-0.635142</td>\n",
       "      <td>1.156048</td>\n",
       "      <td>1.132458</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.692064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.382553</td>\n",
       "      <td>-0.363008</td>\n",
       "      <td>0.650021</td>\n",
       "      <td>-1.790172</td>\n",
       "      <td>0.530437</td>\n",
       "      <td>-1.188590</td>\n",
       "      <td>-0.165349</td>\n",
       "      <td>-0.184930</td>\n",
       "      <td>-1.518766</td>\n",
       "      <td>-0.391453</td>\n",
       "      <td>...</td>\n",
       "      <td>1.730192</td>\n",
       "      <td>-0.683712</td>\n",
       "      <td>0.021395</td>\n",
       "      <td>-0.757571</td>\n",
       "      <td>1.422284</td>\n",
       "      <td>-1.209806</td>\n",
       "      <td>-0.792357</td>\n",
       "      <td>-1.105201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.680140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.603760</td>\n",
       "      <td>0.518553</td>\n",
       "      <td>0.806036</td>\n",
       "      <td>1.595593</td>\n",
       "      <td>0.345827</td>\n",
       "      <td>-0.095435</td>\n",
       "      <td>0.447149</td>\n",
       "      <td>0.386459</td>\n",
       "      <td>0.725640</td>\n",
       "      <td>1.888981</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050841</td>\n",
       "      <td>1.042146</td>\n",
       "      <td>-0.717892</td>\n",
       "      <td>0.081640</td>\n",
       "      <td>1.536767</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>0.755558</td>\n",
       "      <td>-0.574300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.251360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.409157</td>\n",
       "      <td>-1.833109</td>\n",
       "      <td>-0.049377</td>\n",
       "      <td>-0.263717</td>\n",
       "      <td>0.826088</td>\n",
       "      <td>1.071499</td>\n",
       "      <td>-0.322884</td>\n",
       "      <td>0.145183</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>-1.015936</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384283</td>\n",
       "      <td>1.044575</td>\n",
       "      <td>-1.061909</td>\n",
       "      <td>-0.676519</td>\n",
       "      <td>-0.026944</td>\n",
       "      <td>0.087043</td>\n",
       "      <td>0.628350</td>\n",
       "      <td>-0.575092</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.597685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.207963</td>\n",
       "      <td>0.342192</td>\n",
       "      <td>0.663576</td>\n",
       "      <td>-0.221148</td>\n",
       "      <td>0.061020</td>\n",
       "      <td>-0.535162</td>\n",
       "      <td>-0.082077</td>\n",
       "      <td>0.294936</td>\n",
       "      <td>-0.165798</td>\n",
       "      <td>0.870767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566029</td>\n",
       "      <td>-0.686856</td>\n",
       "      <td>0.208857</td>\n",
       "      <td>-1.659950</td>\n",
       "      <td>-0.324920</td>\n",
       "      <td>-1.087536</td>\n",
       "      <td>2.005481</td>\n",
       "      <td>-0.697489</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.794526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.346993</td>\n",
       "      <td>0.068621</td>\n",
       "      <td>0.823924</td>\n",
       "      <td>-0.271435</td>\n",
       "      <td>-2.405479</td>\n",
       "      <td>-0.469236</td>\n",
       "      <td>0.172171</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>-1.547032</td>\n",
       "      <td>-1.796699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359756</td>\n",
       "      <td>1.663935</td>\n",
       "      <td>1.166835</td>\n",
       "      <td>0.222461</td>\n",
       "      <td>-1.093946</td>\n",
       "      <td>1.366550</td>\n",
       "      <td>-0.261986</td>\n",
       "      <td>1.412218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.525261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.053039</td>\n",
       "      <td>0.153148</td>\n",
       "      <td>0.308240</td>\n",
       "      <td>1.341709</td>\n",
       "      <td>0.753734</td>\n",
       "      <td>1.047117</td>\n",
       "      <td>-0.639706</td>\n",
       "      <td>0.260636</td>\n",
       "      <td>1.141632</td>\n",
       "      <td>0.260301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272428</td>\n",
       "      <td>1.409107</td>\n",
       "      <td>0.927301</td>\n",
       "      <td>0.846789</td>\n",
       "      <td>0.132076</td>\n",
       "      <td>-1.401899</td>\n",
       "      <td>-1.561929</td>\n",
       "      <td>0.637778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.670576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.503785</td>\n",
       "      <td>0.239784</td>\n",
       "      <td>-0.463883</td>\n",
       "      <td>0.301699</td>\n",
       "      <td>-1.118219</td>\n",
       "      <td>-1.529466</td>\n",
       "      <td>-0.987392</td>\n",
       "      <td>0.096509</td>\n",
       "      <td>-1.776712</td>\n",
       "      <td>3.240293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601017</td>\n",
       "      <td>-0.428763</td>\n",
       "      <td>0.070027</td>\n",
       "      <td>-0.076321</td>\n",
       "      <td>0.587015</td>\n",
       "      <td>0.944370</td>\n",
       "      <td>-0.134259</td>\n",
       "      <td>-1.759135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.970363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.332729</td>\n",
       "      <td>0.391847</td>\n",
       "      <td>-0.105083</td>\n",
       "      <td>-0.107311</td>\n",
       "      <td>-1.232736</td>\n",
       "      <td>1.423755</td>\n",
       "      <td>0.481433</td>\n",
       "      <td>-0.865070</td>\n",
       "      <td>-1.021945</td>\n",
       "      <td>2.090308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072203</td>\n",
       "      <td>-1.019677</td>\n",
       "      <td>-0.431123</td>\n",
       "      <td>0.410709</td>\n",
       "      <td>0.896270</td>\n",
       "      <td>-0.559364</td>\n",
       "      <td>-0.517319</td>\n",
       "      <td>-0.410625</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.456144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.116123</td>\n",
       "      <td>0.309489</td>\n",
       "      <td>-0.427135</td>\n",
       "      <td>0.647479</td>\n",
       "      <td>0.856144</td>\n",
       "      <td>-0.698585</td>\n",
       "      <td>0.559625</td>\n",
       "      <td>0.437575</td>\n",
       "      <td>-0.015288</td>\n",
       "      <td>0.664923</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.052469</td>\n",
       "      <td>0.279415</td>\n",
       "      <td>0.875625</td>\n",
       "      <td>-0.402659</td>\n",
       "      <td>0.338432</td>\n",
       "      <td>-0.306234</td>\n",
       "      <td>0.989669</td>\n",
       "      <td>0.480751</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.260723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.092582</td>\n",
       "      <td>-0.652052</td>\n",
       "      <td>-0.037670</td>\n",
       "      <td>-1.713115</td>\n",
       "      <td>-0.168013</td>\n",
       "      <td>-0.093245</td>\n",
       "      <td>0.741184</td>\n",
       "      <td>0.321506</td>\n",
       "      <td>1.423107</td>\n",
       "      <td>0.493852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424044</td>\n",
       "      <td>-0.068776</td>\n",
       "      <td>-1.086237</td>\n",
       "      <td>0.139909</td>\n",
       "      <td>0.793890</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>-0.030242</td>\n",
       "      <td>0.075035</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.826796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.322886</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>-0.777154</td>\n",
       "      <td>1.345143</td>\n",
       "      <td>0.549061</td>\n",
       "      <td>0.758271</td>\n",
       "      <td>-1.164498</td>\n",
       "      <td>0.879624</td>\n",
       "      <td>-0.206063</td>\n",
       "      <td>-0.660986</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.202334</td>\n",
       "      <td>0.574118</td>\n",
       "      <td>-0.807763</td>\n",
       "      <td>-0.640016</td>\n",
       "      <td>-0.960385</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.691327</td>\n",
       "      <td>-1.043541</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.072748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.061528</td>\n",
       "      <td>0.376930</td>\n",
       "      <td>-1.023671</td>\n",
       "      <td>0.497743</td>\n",
       "      <td>-0.168200</td>\n",
       "      <td>-1.348586</td>\n",
       "      <td>-0.792008</td>\n",
       "      <td>1.046953</td>\n",
       "      <td>0.286682</td>\n",
       "      <td>1.680488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539881</td>\n",
       "      <td>0.644292</td>\n",
       "      <td>0.446217</td>\n",
       "      <td>-0.040448</td>\n",
       "      <td>1.644592</td>\n",
       "      <td>0.773117</td>\n",
       "      <td>1.479731</td>\n",
       "      <td>-1.434759</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.380715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.117864</td>\n",
       "      <td>-0.019963</td>\n",
       "      <td>0.282497</td>\n",
       "      <td>0.086850</td>\n",
       "      <td>1.845021</td>\n",
       "      <td>0.587779</td>\n",
       "      <td>-0.285588</td>\n",
       "      <td>-0.398168</td>\n",
       "      <td>-0.236866</td>\n",
       "      <td>0.523444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396685</td>\n",
       "      <td>-1.276420</td>\n",
       "      <td>-0.098374</td>\n",
       "      <td>-2.282420</td>\n",
       "      <td>-0.370940</td>\n",
       "      <td>-0.121709</td>\n",
       "      <td>0.380576</td>\n",
       "      <td>-1.628613</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.474129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.466469</td>\n",
       "      <td>-0.252015</td>\n",
       "      <td>-2.265768</td>\n",
       "      <td>0.450651</td>\n",
       "      <td>-0.441777</td>\n",
       "      <td>0.711138</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>2.628471</td>\n",
       "      <td>0.592953</td>\n",
       "      <td>-0.522155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.952684</td>\n",
       "      <td>0.384855</td>\n",
       "      <td>-0.463517</td>\n",
       "      <td>-0.085184</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.842218</td>\n",
       "      <td>0.837008</td>\n",
       "      <td>0.402481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.048422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.683606</td>\n",
       "      <td>-1.263744</td>\n",
       "      <td>0.322668</td>\n",
       "      <td>1.750527</td>\n",
       "      <td>0.823344</td>\n",
       "      <td>-0.701176</td>\n",
       "      <td>-0.427964</td>\n",
       "      <td>-0.730980</td>\n",
       "      <td>1.697011</td>\n",
       "      <td>-2.766386</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.566980</td>\n",
       "      <td>-0.736090</td>\n",
       "      <td>-0.412988</td>\n",
       "      <td>-0.220407</td>\n",
       "      <td>1.801352</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>-1.920599</td>\n",
       "      <td>-1.489814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.068223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.129545</td>\n",
       "      <td>0.329623</td>\n",
       "      <td>-1.279222</td>\n",
       "      <td>-0.031133</td>\n",
       "      <td>-1.955437</td>\n",
       "      <td>-0.632942</td>\n",
       "      <td>0.531645</td>\n",
       "      <td>1.187918</td>\n",
       "      <td>-0.678763</td>\n",
       "      <td>-1.365456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212880</td>\n",
       "      <td>0.471386</td>\n",
       "      <td>0.579322</td>\n",
       "      <td>-0.111222</td>\n",
       "      <td>0.385074</td>\n",
       "      <td>-2.147295</td>\n",
       "      <td>1.278649</td>\n",
       "      <td>0.381729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.879782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.738855</td>\n",
       "      <td>-0.589885</td>\n",
       "      <td>1.483941</td>\n",
       "      <td>-0.492343</td>\n",
       "      <td>-0.945829</td>\n",
       "      <td>0.436821</td>\n",
       "      <td>0.169156</td>\n",
       "      <td>1.408270</td>\n",
       "      <td>0.861501</td>\n",
       "      <td>0.883293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285585</td>\n",
       "      <td>0.609605</td>\n",
       "      <td>-0.173855</td>\n",
       "      <td>-0.844783</td>\n",
       "      <td>-0.386174</td>\n",
       "      <td>-1.700016</td>\n",
       "      <td>0.789026</td>\n",
       "      <td>-0.941432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.688996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.082919</td>\n",
       "      <td>-0.283673</td>\n",
       "      <td>-0.932456</td>\n",
       "      <td>1.253132</td>\n",
       "      <td>1.502478</td>\n",
       "      <td>0.084874</td>\n",
       "      <td>0.218974</td>\n",
       "      <td>-2.135409</td>\n",
       "      <td>-0.655009</td>\n",
       "      <td>0.638137</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.081968</td>\n",
       "      <td>0.688628</td>\n",
       "      <td>1.438495</td>\n",
       "      <td>0.633828</td>\n",
       "      <td>-0.345086</td>\n",
       "      <td>-0.224780</td>\n",
       "      <td>0.115460</td>\n",
       "      <td>-1.516633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.715778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.528681</td>\n",
       "      <td>0.104134</td>\n",
       "      <td>-0.224793</td>\n",
       "      <td>0.311613</td>\n",
       "      <td>-0.258088</td>\n",
       "      <td>1.888570</td>\n",
       "      <td>-0.800467</td>\n",
       "      <td>-2.411364</td>\n",
       "      <td>...</td>\n",
       "      <td>1.465941</td>\n",
       "      <td>-0.267678</td>\n",
       "      <td>-0.030028</td>\n",
       "      <td>-0.674158</td>\n",
       "      <td>0.366959</td>\n",
       "      <td>-1.019553</td>\n",
       "      <td>0.174613</td>\n",
       "      <td>0.345712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.628821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.886345</td>\n",
       "      <td>-0.110938</td>\n",
       "      <td>-0.193629</td>\n",
       "      <td>1.258769</td>\n",
       "      <td>0.401721</td>\n",
       "      <td>0.513008</td>\n",
       "      <td>-0.407618</td>\n",
       "      <td>-0.941499</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277493</td>\n",
       "      <td>0.317899</td>\n",
       "      <td>0.791247</td>\n",
       "      <td>0.688675</td>\n",
       "      <td>0.454456</td>\n",
       "      <td>-1.076939</td>\n",
       "      <td>1.386565</td>\n",
       "      <td>1.266872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.560679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.011209</td>\n",
       "      <td>0.571841</td>\n",
       "      <td>1.236584</td>\n",
       "      <td>0.149405</td>\n",
       "      <td>0.857720</td>\n",
       "      <td>1.604200</td>\n",
       "      <td>0.473612</td>\n",
       "      <td>0.024652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.857875</td>\n",
       "      <td>-1.267965</td>\n",
       "      <td>-0.500968</td>\n",
       "      <td>1.059198</td>\n",
       "      <td>-0.479415</td>\n",
       "      <td>-1.379230</td>\n",
       "      <td>0.065363</td>\n",
       "      <td>-0.917905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.970864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.201213</td>\n",
       "      <td>-0.595170</td>\n",
       "      <td>-2.322639</td>\n",
       "      <td>0.446249</td>\n",
       "      <td>-0.834830</td>\n",
       "      <td>-1.033700</td>\n",
       "      <td>-0.336003</td>\n",
       "      <td>0.086409</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057340</td>\n",
       "      <td>0.626318</td>\n",
       "      <td>1.588044</td>\n",
       "      <td>-2.821950</td>\n",
       "      <td>0.491804</td>\n",
       "      <td>0.177488</td>\n",
       "      <td>1.032223</td>\n",
       "      <td>1.019005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.739711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.219492</td>\n",
       "      <td>-1.602644</td>\n",
       "      <td>1.192015</td>\n",
       "      <td>0.899695</td>\n",
       "      <td>0.560520</td>\n",
       "      <td>1.531837</td>\n",
       "      <td>-0.777305</td>\n",
       "      <td>-0.610006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799064</td>\n",
       "      <td>1.251809</td>\n",
       "      <td>2.105218</td>\n",
       "      <td>0.932020</td>\n",
       "      <td>0.556843</td>\n",
       "      <td>-0.512615</td>\n",
       "      <td>-0.694896</td>\n",
       "      <td>3.896194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.080477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -0.992353  0.323965 -0.302274 -1.585237 -0.544800 -0.000248 -0.021441   \n",
       "1   1.298005  1.169075  0.078595  0.186472  0.009394 -0.077167  0.935385   \n",
       "2  -0.786382 -0.463325  0.721203  1.348425 -0.501516  0.488126  1.598850   \n",
       "3  -2.276180  0.545390  1.035878 -0.755436  0.430923  0.669475 -0.537089   \n",
       "4  -1.259395 -1.594934  1.809248  1.002226 -0.815308  1.749017  0.493437   \n",
       "5  -0.415712  1.443924 -0.920818  2.263012  1.571203  0.744302 -1.402939   \n",
       "6  -0.182928  0.111827 -1.209468  0.021120 -1.185045  0.461112 -1.344325   \n",
       "7   0.641366 -0.289803  0.694779  1.942122  1.372276 -1.522415 -0.673062   \n",
       "8  -1.191516  0.576992 -1.643182 -0.131669  0.343353 -1.711821 -0.443054   \n",
       "9  -2.311907  1.210484  0.801909  0.518531 -0.531715 -0.010624 -0.408591   \n",
       "10 -1.784797 -1.849507 -0.581304 -1.219007  0.180560 -1.475992 -1.396014   \n",
       "11  0.819581 -0.007059 -0.097815  1.864404  1.052482  0.423650  1.379318   \n",
       "12  0.382553 -0.363008  0.650021 -1.790172  0.530437 -1.188590 -0.165349   \n",
       "13  0.603760  0.518553  0.806036  1.595593  0.345827 -0.095435  0.447149   \n",
       "14 -0.409157 -1.833109 -0.049377 -0.263717  0.826088  1.071499 -0.322884   \n",
       "15 -0.207963  0.342192  0.663576 -0.221148  0.061020 -0.535162 -0.082077   \n",
       "16 -0.346993  0.068621  0.823924 -0.271435 -2.405479 -0.469236  0.172171   \n",
       "17 -0.053039  0.153148  0.308240  1.341709  0.753734  1.047117 -0.639706   \n",
       "18  0.503785  0.239784 -0.463883  0.301699 -1.118219 -1.529466 -0.987392   \n",
       "19  1.332729  0.391847 -0.105083 -0.107311 -1.232736  1.423755  0.481433   \n",
       "20  0.116123  0.309489 -0.427135  0.647479  0.856144 -0.698585  0.559625   \n",
       "21 -0.092582 -0.652052 -0.037670 -1.713115 -0.168013 -0.093245  0.741184   \n",
       "22 -1.322886  0.907937 -0.777154  1.345143  0.549061  0.758271 -1.164498   \n",
       "23  0.061528  0.376930 -1.023671  0.497743 -0.168200 -1.348586 -0.792008   \n",
       "24 -1.117864 -0.019963  0.282497  0.086850  1.845021  0.587779 -0.285588   \n",
       "25 -0.466469 -0.252015 -2.265768  0.450651 -0.441777  0.711138  0.994005   \n",
       "26 -0.683606 -1.263744  0.322668  1.750527  0.823344 -0.701176 -0.427964   \n",
       "27  0.129545  0.329623 -1.279222 -0.031133 -1.955437 -0.632942  0.531645   \n",
       "28 -0.738855 -0.589885  1.483941 -0.492343 -0.945829  0.436821  0.169156   \n",
       "29  1.082919 -0.283673 -0.932456  1.253132  1.502478  0.084874  0.218974   \n",
       "30       NaN       NaN  1.528681  0.104134 -0.224793  0.311613 -0.258088   \n",
       "31       NaN       NaN -0.886345 -0.110938 -0.193629  1.258769  0.401721   \n",
       "32       NaN       NaN -0.011209  0.571841  1.236584  0.149405  0.857720   \n",
       "33       NaN       NaN -0.201213 -0.595170 -2.322639  0.446249 -0.834830   \n",
       "34       NaN       NaN -0.219492 -1.602644  1.192015  0.899695  0.560520   \n",
       "\n",
       "           7         8         9  ...        32        33        34        35  \\\n",
       "0   0.316169 -0.098997  1.085379  ... -0.888431 -0.119908  0.580787 -0.399038   \n",
       "1  -0.559335  0.341219  0.803685  ... -0.879894  1.283240  0.275753  0.412148   \n",
       "2   0.223581 -0.727041 -0.928016  ... -1.998879  0.021455 -0.213586  0.838839   \n",
       "3   0.515443  0.047906 -1.174832  ... -1.110335 -0.416424 -0.862757  1.326787   \n",
       "4   0.433594 -0.762308  0.363637  ... -1.790596  0.929426  0.428156 -0.882503   \n",
       "5  -1.623268  2.487828  2.005971  ... -0.068997 -0.620042  2.154509 -1.618542   \n",
       "6  -2.157952  0.752266 -0.063163  ... -1.309927 -0.206375  1.288428 -1.122680   \n",
       "7   0.305787 -0.121505  0.085683  ...  0.865407 -0.557632  0.189961  0.819096   \n",
       "8  -0.634584  1.708412  1.385935  ... -0.949156  1.201457  1.199512 -0.423426   \n",
       "9  -1.236370 -0.386644 -0.745807  ...  0.290238 -0.467597  0.198447  0.575039   \n",
       "10  0.797542 -1.867400 -0.916721  ...  0.514531 -1.546215 -0.365697 -1.512766   \n",
       "11 -0.669502  2.061232 -1.467686  ...  1.214861  1.343246 -0.648931  0.973022   \n",
       "12 -0.184930 -1.518766 -0.391453  ...  1.730192 -0.683712  0.021395 -0.757571   \n",
       "13  0.386459  0.725640  1.888981  ...  1.050841  1.042146 -0.717892  0.081640   \n",
       "14  0.145183  0.911600 -1.015936  ...  1.384283  1.044575 -1.061909 -0.676519   \n",
       "15  0.294936 -0.165798  0.870767  ... -0.566029 -0.686856  0.208857 -1.659950   \n",
       "16  0.843373 -1.547032 -1.796699  ... -0.359756  1.663935  1.166835  0.222461   \n",
       "17  0.260636  1.141632  0.260301  ...  0.272428  1.409107  0.927301  0.846789   \n",
       "18  0.096509 -1.776712  3.240293  ...  0.601017 -0.428763  0.070027 -0.076321   \n",
       "19 -0.865070 -1.021945  2.090308  ... -0.072203 -1.019677 -0.431123  0.410709   \n",
       "20  0.437575 -0.015288  0.664923  ... -1.052469  0.279415  0.875625 -0.402659   \n",
       "21  0.321506  1.423107  0.493852  ... -0.424044 -0.068776 -1.086237  0.139909   \n",
       "22  0.879624 -0.206063 -0.660986  ... -3.202334  0.574118 -0.807763 -0.640016   \n",
       "23  1.046953  0.286682  1.680488  ... -0.539881  0.644292  0.446217 -0.040448   \n",
       "24 -0.398168 -0.236866  0.523444  ...  1.396685 -1.276420 -0.098374 -2.282420   \n",
       "25  2.628471  0.592953 -0.522155  ... -0.952684  0.384855 -0.463517 -0.085184   \n",
       "26 -0.730980  1.697011 -2.766386  ... -1.566980 -0.736090 -0.412988 -0.220407   \n",
       "27  1.187918 -0.678763 -1.365456  ...  0.212880  0.471386  0.579322 -0.111222   \n",
       "28  1.408270  0.861501  0.883293  ...  0.285585  0.609605 -0.173855 -0.844783   \n",
       "29 -2.135409 -0.655009  0.638137  ... -2.081968  0.688628  1.438495  0.633828   \n",
       "30  1.888570 -0.800467 -2.411364  ...  1.465941 -0.267678 -0.030028 -0.674158   \n",
       "31  0.513008 -0.407618 -0.941499  ... -0.277493  0.317899  0.791247  0.688675   \n",
       "32  1.604200  0.473612  0.024652  ...  1.857875 -1.267965 -0.500968  1.059198   \n",
       "33 -1.033700 -0.336003  0.086409  ...  1.057340  0.626318  1.588044 -2.821950   \n",
       "34  1.531837 -0.777305 -0.610006  ...  0.799064  1.251809  2.105218  0.932020   \n",
       "\n",
       "          36        37        38        39  one        sum  \n",
       "0   1.200697  2.305707  0.531889  1.483389  1.0   1.378333  \n",
       "1  -0.494896  2.457624  0.238578 -1.975909  1.0   9.055729  \n",
       "2   0.124605 -0.246737 -1.024014  0.612363  1.0  -7.889502  \n",
       "3   0.690854 -0.906616  0.476509  0.699091  1.0   0.072863  \n",
       "4   0.082432 -1.320863  0.693618  1.953759  1.0   2.294877  \n",
       "5   0.785786 -1.704877  0.118322  0.522214  1.0  -0.051083  \n",
       "6  -0.191745 -0.129728 -0.132934 -0.470554  1.0   1.206534  \n",
       "7   0.095489  1.006587  0.793813  0.899724  1.0  10.180246  \n",
       "8  -0.421269  0.505954 -1.413370 -0.839522  1.0  -5.540943  \n",
       "9  -0.325595 -1.779368 -1.016658  0.899743  1.0  -8.831396  \n",
       "10 -0.365535 -0.509073 -2.063466  0.436263  1.0 -25.330382  \n",
       "11  1.266891 -0.635142  1.156048  1.132458  1.0   8.692064  \n",
       "12  1.422284 -1.209806 -0.792357 -1.105201  1.0 -11.680140  \n",
       "13  1.536767 -0.014950  0.755558 -0.574300  1.0  14.251360  \n",
       "14 -0.026944  0.087043  0.628350 -0.575092  1.0   0.597685  \n",
       "15 -0.324920 -1.087536  2.005481 -0.697489  1.0  -1.794526  \n",
       "16 -1.093946  1.366550 -0.261986  1.412218  1.0   0.525261  \n",
       "17  0.132076 -1.401899 -1.561929  0.637778  1.0   8.670576  \n",
       "18  0.587015  0.944370 -0.134259 -1.759135  1.0   1.970363  \n",
       "19  0.896270 -0.559364 -0.517319 -0.410625  1.0   6.456144  \n",
       "20  0.338432 -0.306234  0.989669  0.480751  1.0  -4.260723  \n",
       "21  0.793890  0.020622 -0.030242  0.075035  1.0  -1.826796  \n",
       "22 -0.960385  0.003131  0.691327 -1.043541  1.0  -5.072748  \n",
       "23  1.644592  0.773117  1.479731 -1.434759  1.0   7.380715  \n",
       "24 -0.370940 -0.121709  0.380576 -1.628613  1.0   1.474129  \n",
       "25  0.095669  0.842218  0.837008  0.402481  1.0   4.048422  \n",
       "26  1.801352  0.467427 -1.920599 -1.489814  1.0   1.068223  \n",
       "27  0.385074 -2.147295  1.278649  0.381729  1.0   2.879782  \n",
       "28 -0.386174 -1.700016  0.789026 -0.941432  1.0  -0.688996  \n",
       "29 -0.345086 -0.224780  0.115460 -1.516633  1.0   0.715778  \n",
       "30  0.366959 -1.019553  0.174613  0.345712  NaN  -1.628821  \n",
       "31  0.454456 -1.076939  1.386565  1.266872  NaN   0.560679  \n",
       "32 -0.479415 -1.379230  0.065363 -0.917905  NaN   0.970864  \n",
       "33  0.491804  0.177488  1.032223  1.019005  NaN  -4.739711  \n",
       "34  0.556843 -0.512615 -0.694896  3.896194  NaN   4.080477  \n",
       "\n",
       "[35 rows x 42 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from pandas import DataFrame\n",
    "from numpy import random, nan, ones\n",
    "\n",
    "T = 30\n",
    "h = 5\n",
    "num_free_vars = 40\n",
    "df_true = DataFrame(random.normal(0,1,[T+h,num_free_vars]))\n",
    "df_true['one'] = 1 # constant\n",
    "df_true['sum'] = df_true.iloc[:,:].sum(axis=1)\n",
    "\n",
    "# constraint in true data\n",
    "df_true['sum']-df_true.iloc[:,:-1].sum(axis=1)\n",
    "\n",
    "num_variables = num_free_vars + 2\n",
    "\n",
    "df = df_true.copy()\n",
    "df[0].iloc[-h:] = nan\n",
    "df[1].iloc[-h:] = nan\n",
    "df['one'].iloc[-h:] = nan\n",
    "C = ones([1,num_variables])\n",
    "C[0,-1] = -1\n",
    "d = 0\n",
    "C_dict = {}\n",
    "d_dict = {}\n",
    "for i in range(T+h):\n",
    "    C_dict[i] = C\n",
    "    d_dict[i] = d\n",
    "\n",
    "lag = 1\n",
    "Tin = 5\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mff.mff import ax_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df and C are re-ordered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) has score: 0.5583539064520213\n",
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('naive',\n",
      "                                                      NaiveForecaster())]),\n",
      "                           transformer=StandardScaler()) has score: 0.8743714417941255\n",
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 0.791589111131416\n",
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_kernel',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('kernel_ridge',\n",
      "                                                                                                    KernelRidge(kernel='rbf'))]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'kernel_ridge__alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd66710>,\n",
      "                                                                                              'kernel_ridge__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65410>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.5591587866295123\n",
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_svr',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('svr',\n",
      "                                                                                                    SVR())]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65a10>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.6472840056240162\n",
      "For variable 0 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) with score: 0.6472840056240162\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) has score: 0.561955716278754\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('naive',\n",
      "                                                      NaiveForecaster())]),\n",
      "                           transformer=StandardScaler()) has score: 0.8125734713383217\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 0.738405976784996\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_kernel',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('kernel_ridge',\n",
      "                                                                                                    KernelRidge(kernel='rbf'))]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'kernel_ridge__alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd66710>,\n",
      "                                                                                              'kernel_ridge__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65410>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.5698674569122285\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_svr',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('svr',\n",
      "                                                                                                    SVR())]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65a10>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.646899119492452\n",
      "For variable 1 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) with score: 0.646899119492452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/macair/miniconda3/envs/mff-dev/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 0.000e+00, tolerance: 0.000e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('naive',\n",
      "                                                      NaiveForecaster())]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_kernel',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('kernel_ridge',\n",
      "                                                                                                    KernelRidge(kernel='rbf'))]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'kernel_ridge__alpha': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd66710>,\n",
      "                                                                                              'kernel_ridge__gamma': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65410>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('randomsearch_cv_svr',\n",
      "                                                      RandomizedSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                         estimator=Pipeline(steps=[('scaler',\n",
      "                                                                                                    StandardScaler()),\n",
      "                                                                                                   ('svr',\n",
      "                                                                                                    SVR())]),\n",
      "                                                                         n_iter=500,\n",
      "                                                                         param_distributions={'svr__C': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x14fd65a10>},\n",
      "                                                                         random_state=0))]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('elasticnet',\n",
      "                                                      ElasticNetCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None),\n",
      "                                                                   fit_intercept=False,\n",
      "                                                                   max_iter=500))]),\n",
      "                           transformer=StandardScaler()) with score: 0.0\n",
      "time 22.92538809776306\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df2,df1,df0aug_coef = ax_forecast(df, lag, Tin, C_dict, d_dict)\n",
    "end = time.time()\n",
    "print('time',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import concat\n",
    "\n",
    "concat([df_true[1], df2[1]], axis=1).tail(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df and C are re-ordered\n",
      "For variable 0,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 1.1128241971969381\n",
      "For variable 0 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) with score: 1.1128241971969381\n",
      "For variable 1,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 1.0449029779200334\n",
      "For variable 1 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) with score: 1.0449029779200334\n",
      "For variable 2,  model TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) has score: 0.0\n",
      "For variable 2 the best model is TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n",
      "                                                      StandardScaler()),\n",
      "                                                     ('PCA',\n",
      "                                                      PCA(n_components=0.9)),\n",
      "                                                     ('linreg',\n",
      "                                                      LinearRegression(fit_intercept=False))]),\n",
      "                           transformer=StandardScaler()) with score: 0.0\n",
      "time 0.13965106010437012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_linear_regression = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"PCA\", PCA(n_components=0.9)),\n",
    "        (\"linreg\", LinearRegression(fit_intercept=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "df2,df1,df0aug_coef = ax_forecast(df, lag, Tin, C_dict, d_dict, estimators=pipeline_linear_regression)\n",
    "end = time.time()\n",
    "print('time',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import concat\n",
    "var_to_show = 0\n",
    "\n",
    "concat([df_true[var_to_show], df1[var_to_show], df2[var_to_show]], axis=1).tail(10).plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mff-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
